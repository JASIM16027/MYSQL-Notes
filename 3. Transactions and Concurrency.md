---

## **3. Transactions and Concurrency**
- Explain the difference between **pessimistic locking** and **optimistic locking**.
- What are database isolation levels? Can you explain the differences between **READ UNCOMMITTED**, **READ COMMITTED**, **REPEATABLE READ**, and **SERIALIZABLE**?
- How would you handle **deadlocks** in a database?
- What is a **two-phase commit**? When would you use it?

---



## Explain the difference between **pessimistic locking** and **optimistic locking**.

Pessimistic locking and optimistic locking are strategies used in database management to handle **concurrent data access** and ensure **data integrity** when multiple users or processes interact with the same dataset at the same time. The right choice between these two approaches depends on the specific use case, system design, and level of contention in the environment.

---

### **1. What is Pessimistic Locking?**

#### **Definition:**
Pessimistic locking assumes that conflicts between concurrent transactions **are likely to occur**. It prevents these conflicts by locking the data immediately when it is accessed, ensuring that only one transaction can work on it at a time.

---

#### **How It Works:**
- When a transaction accesses a piece of data, it locks the record (shared or exclusive lock).
- Other transactions attempting to access the same record must either:
  - Wait until the lock is released.
  - Fail if they cannot acquire the lock in time.

---

#### **Example Use Case:**
Imagine two processes trying to update an inventory table:
1. **Transaction 1:**
   ```sql
   BEGIN TRANSACTION;
   SELECT * FROM Inventory WHERE ProductID = 101 FOR UPDATE;
   ```
   - Locks the row with `ProductID = 101`, preventing other updates.

2. **Transaction 2:**
   ```sql
   SELECT * FROM Inventory WHERE ProductID = 101 FOR UPDATE;
   ```
   - This query is **blocked** until Transaction 1 releases the lock.

---

#### **Advantages of Pessimistic Locking:**

1. **Data Integrity Guaranteed:** Conflicts are avoided as only one transaction can modify the data at a time.
2. **No Need for Conflict Detection:** Since conflicts are prevented, there’s no need to check for version changes.
3. Ideal for **high-risk systems**, such as:
   - Banking systems (e.g., transferring money).
   - Real-time inventory systems.

---

#### **Disadvantages of Pessimistic Locking:**

1. **Performance Bottlenecks:** Other transactions must wait, reducing concurrency.
2. **Deadlocks:** Two or more transactions can block each other indefinitely while waiting for locks.
3. **Overhead:** Managing locks increases resource usage and slows down the system in high-traffic scenarios.

---

---

### **2. What is Optimistic Locking?**

#### **Definition:**
Optimistic locking assumes that conflicts are **rare** and allows transactions to proceed without locking the data. Instead, it verifies at the time of committing changes whether another transaction has modified the data.

---

#### **How It Works:**
1. Each data record has a **version identifier** (e.g., a version number or timestamp).
2. A transaction reads the data along with its version.
3. Before committing updates:
   - The transaction checks whether the version is the same as it was when the data was first read.
   - If the version is unchanged, the update proceeds, and the version is incremented.
   - If the version has changed, the transaction fails and must be retried.

---

#### **Example Use Case:**
A `Products` table has a `Version` column.

1. **Transaction 1: Reads the Record**
   ```sql
   SELECT ProductID, Stock, Version FROM Products WHERE ProductID = 1;
   ```
   - Reads the row with `Version = 5`.

2. **Transaction 2: Updates the Same Row**
   ```sql
   UPDATE Products
   SET Stock = Stock - 10, Version = Version + 1
   WHERE ProductID = 1 AND Version = 5;
   ```
   - Changes the version to `6`.

3. **Transaction 1: Tries to Update**
   ```sql
   UPDATE Products
   SET Stock = Stock - 5, Version = Version + 1
   WHERE ProductID = 1 AND Version = 5;
   ```
   - **Fails** because the version has changed to `6`.

4. **Transaction 1: Retries After Reading the Updated Data**
   ```sql
   SELECT ProductID, Stock, Version FROM Products WHERE ProductID = 1;
   ```
   - Retrieves the latest data and retries the operation.

---

#### **Advantages of Optimistic Locking:**
1. **High Concurrency:** No locks mean multiple transactions can proceed simultaneously.
2. **No Deadlocks:** Since no locks are held, circular waiting cannot occur.
3. Ideal for **low-contention systems**, such as:
   - Reporting or analytics systems.
   - Applications with a high volume of read operations and infrequent writes.

---

#### **Disadvantages of Optimistic Locking:**
1. **Retry Overhead:** Transactions may fail if there is a version mismatch, requiring retries.
2. **Conflict Detection Costs:** Managing and verifying version numbers adds computational overhead.
3. **Not Suitable for High Contention:** In systems with frequent data updates, retries may occur too often, reducing efficiency.

---

---

### **Key Differences Between Pessimistic and Optimistic Locking**

| **Feature**             | **Pessimistic Locking**                               | **Optimistic Locking**                               |
|--------------------------|------------------------------------------------------|-----------------------------------------------------|
| **Conflict Handling**    | Prevents conflicts by locking data upfront.          | Detects conflicts during commit time.               |
| **Concurrency**          | Low concurrency due to locked data.                  | High concurrency as no locks are used.              |
| **Deadlocks**            | High risk of deadlocks due to locks.                 | No risk of deadlocks since no locks are used.       |
| **Performance**          | Slower due to blocking transactions.                 | Faster but may require retries.                     |
| **Overhead**             | Lock management adds overhead.                       | Version maintenance adds overhead.                  |
| **Use Cases**            | High-contention, write-heavy systems (e.g., banks).  | Low-contention, read-heavy systems (e.g., reports). |
| **Data Integrity**       | Ensures consistency upfront.                         | Relies on versioning to maintain consistency.        |

---

### **How to Choose Between Pessimistic and Optimistic Locking**

#### Use **Pessimistic Locking** When:
- **High contention** is expected, and conflicts are frequent.
- Data consistency is **critical**, and retries are not acceptable (e.g., money transfers).
- **Write-heavy systems** where simultaneous updates are common.

#### Use **Optimistic Locking** When:
- Conflicts are **rare**, and the system can tolerate retries.
- The application is **read-heavy** with occasional writes.
- **High performance and scalability** are priorities, such as in e-commerce or analytics systems.

---

### **Summary**
- **Pessimistic Locking:** Prevents conflicts by locking data but reduces performance and increases the risk of deadlocks. Suitable for environments with frequent updates and critical consistency requirements.
- **Optimistic Locking:** Detects conflicts later, offering better concurrency but requiring mechanisms to handle retries. Best suited for environments with low contention and high read activity.

Choosing the right strategy depends on the application’s needs, contention levels, and trade-offs between performance and data consistency.



## How would you handle **deadlocks** in a database?

### **Handling Deadlocks in a Database**

Handling deadlocks in a database is a critical aspect of ensuring system reliability and performance. Deadlocks occur when two or more transactions are waiting indefinitely for one another to release locks on resources, creating a cycle of dependencies that cannot be resolved. Below is a detailed explanation of how to handle deadlocks:

---

To effectively handle deadlocks, we can use **prevention**, **detection**, and **resolution** techniques.

### **1. Understanding Deadlocks**
A deadlock arises when the following four conditions (known as the Coffman conditions) are met:
1. **Mutual Exclusion**: Only one transaction can hold a lock on a resource at a time.
2. **Hold and Wait**: A transaction holds a lock on a resource while waiting for another resource.
3. **No Preemption**: A lock cannot be forcibly taken away from a transaction; it must be released voluntarily.
4. **Circular Wait**: A cycle of transactions exists where each transaction is waiting for a resource held by the next transaction in the cycle.

---

### **2. Deadlock Prevention**
Preventing deadlocks involves ensuring that at least one of the Coffman conditions cannot occur. Common strategies include:

### **a. Lock Ordering (Consistent Order of Resource Acquisition)**
- Ensure that transactions acquire locks in a consistent order.
- Example: If Transaction A needs to lock Table X and then Table Y, Transaction B must also lock Table X before Table Y.
- This prevents circular waits.

### **b. Timeout-Based Abort (Setting Lock Timeout)**
- Set a **lock timeout** so that a transaction will not wait indefinitely.
- Set a maximum time a transaction can wait for a lock. If the timeout is reached, the transaction is rolled back.
- This breaks the "hold and wait" condition.
- Example (PostgreSQL):
  
  ```sql
  
  SET lock_timeout = '5s';

  ```

- If a transaction cannot acquire a lock within 5 seconds, it aborts and retries.


#### **c. Preemption**
- Allow the database to forcibly revoke locks from one transaction to resolve a deadlock.
- This violates the "no preemption" condition.

#### **d. Wait-Die or Wound-Wait Schemes**

- **Wait-Die**: If a transaction requests a lock held by an older transaction, it waits. If the requesting transaction is older, it is aborted.
- **Wound-Wait**: If a transaction requests a lock held by a younger transaction, the younger transaction is aborted. If the requesting transaction is younger, it waits.
- These schemes prevent circular waits by prioritizing older transactions.

---

### **Avoid Long Transactions**
- Shorter transactions reduce the chance of deadlocks.
- Use **batch processing** instead of large, long-running transactions.

### ** Using Lower Isolation Levels (When Possible)**
- Higher isolation levels (e.g., **Serializable**) increase the chance of deadlocks.
- Instead, use **Read Committed** or **Repeatable Read** when possible.

---

## **2. Deadlock Detection**
If prevention is not feasible, databases must detect and resolve deadlocks.

#### **a. Wait-for Graph**
- A directed graph where nodes represent transactions, and edges represent dependencies (e.g., Transaction A is waiting for a lock held by Transaction B).
- The database periodically checks for cycles in the graph. If a cycle is found, a deadlock exists.

#### **b. Timeout Mechanisms**
- If a transaction waits too long for a lock, it is assumed to be part of a deadlock and is rolled back.

---

### **a. Deadlock Detection in Database Management Systems (DBMS)**
Most databases have built-in deadlock detection mechanisms:
- **PostgreSQL:** Uses a background process to detect circular waits.
- **MySQL (InnoDB):** Detects deadlocks when transactions wait for locks.
- **SQL Server:** Runs a deadlock monitor periodically.

### **b. Example: Checking for Deadlocks in MySQL**
You can use the following command to check deadlocks:
```sql
SHOW ENGINE INNODB STATUS;
```
This will output the latest deadlock details.

---

## **3. Deadlock Resolution**
Once a deadlock is detected, the DBMS automatically resolves it using **one of the following strategies**:

### **a. Transaction Rollback (Victim Selection)**
- The DBMS **selects one transaction** (usually the one with the least impact) and rolls it back.
- The aborted transaction can then retry.

### **b. Retrying the Transaction with Exponential Backoff**
- If a deadlock occurs, the application can **retry the transaction after a short delay**.
- Example in **Node.js with TypeORM**:
  ```typescript
  async function executeTransaction(entityManager) {
    const MAX_RETRIES = 3;
    let attempt = 0;

    while (attempt < MAX_RETRIES) {
      try {
        return await entityManager.transaction(async (transactionalEntityManager) => {
          // Perform operations
        });
      } catch (error) {
        if (error.message.includes('Deadlock')) {
          attempt++;
          await new Promise((resolve) => setTimeout(resolve, Math.pow(2, attempt) * 100)); // Exponential backoff
        } else {
          throw error;
        }
      }
    }
  }
  ```
- The **exponential backoff** reduces contention by delaying the retry.

### **c. Using Optimistic Concurrency Control (OCC)**
Instead of locking rows, use **version numbers** or timestamps:
- A transaction updates only if the record has not been modified by another transaction.
- Example (PostgreSQL):
  ```sql
  UPDATE employees 
  SET salary = salary + 1000 
  WHERE id = 1 AND last_updated = '2025-01-30T10:00:00';
  ```
- If `last_updated` has changed, the transaction is aborted and retried.

---


### **5. Best Practices to Minimize Deadlocks**
- **Keep Transactions Short**: Minimize the time locks are held by reducing transaction duration.
- **Access Resources in a Consistent Order**: Ensure all transactions access tables or rows in the same sequence.
- **Use Indexes**: Proper indexing reduces the likelihood of locking large portions of a table.
- **Limit Lock Granularity**: Use row-level locks instead of table-level locks where possible.
- **Monitor and Tune**: Regularly monitor the database for deadlocks and adjust application logic or database configuration as needed.

---

### **6. Database-Specific Mechanisms**
Different databases provide built-in mechanisms for handling deadlocks:
- **MySQL**: Automatically detects deadlocks and rolls back the transaction with the least impact.
- **PostgreSQL**: Uses a wait-for graph for detection and rolls back one of the involved transactions.
- **Oracle**: Employs a timeout-based mechanism and provides detailed deadlock information in trace files.
- **SQL Server**: Detects deadlocks using a dedicated monitor thread and provides deadlock graphs for analysis.

---

### **7. Application-Level Handling**
- **Retry Logic**: Implement retry logic in the application to handle deadlocks gracefully.
- **Error Handling**: Catch deadlock-related errors (e.g., `SQLSTATE 40001` in SQL for deadlocks) and retry the transaction.
- **Logging**: Log deadlock occurrences for analysis and optimization.

---

## **Conclusion**
To handle deadlocks effectively:
1. **Prevent them** using consistent locking order, timeouts, and shorter transactions.
2. **Detect them** using DBMS logs (`SHOW ENGINE INNODB STATUS`).
3. **Resolve them** by rolling back and retrying transactions with exponential backoff.


### **Conclusion**
Deadlocks are an inherent challenge in concurrent database systems, but they can be managed effectively through a combination of prevention, detection, and resolution strategies. By understanding the underlying causes and implementing best practices, you can minimize the occurrence of deadlocks and ensure the smooth operation of your database system.


## **Exponential Backoff: A Strategy for Handling Deadlocks and Retrying Transactions**  

#### **What is Exponential Backoff?**  
Exponential backoff is a strategy where, after a failure (such as a deadlock), the system **waits for an exponentially increasing amount of time** before retrying the operation. This reduces contention and improves overall system stability.

Instead of retrying immediately, we **increase the delay exponentially** (e.g., 100ms, 200ms, 400ms, 800ms, etc.) to avoid overwhelming the system.

---

### **How Exponential Backoff Works in Deadlock Handling**
1. A transaction tries to acquire a lock.
2. If a deadlock occurs, the DBMS **rolls back the transaction**.
3. Instead of retrying immediately, the transaction **waits for a random, exponentially increasing delay**.
4. The transaction retries up to a maximum number of attempts.

---

### **Example: Exponential Backoff in Node.js (TypeORM + PostgreSQL/MySQL)**  
This function **retries a transaction up to 5 times**, doubling the wait time after each failure:

```typescript
import { DataSource, EntityManager } from "typeorm";

async function executeWithExponentialBackoff(
  dataSource: DataSource,
  operation: (transactionalEntityManager: EntityManager) => Promise<void>,
  maxRetries: number = 5
) {
  let attempt = 0;

  while (attempt < maxRetries) {
    try {
      await dataSource.transaction(async (transactionalEntityManager) => {
        await operation(transactionalEntityManager);
      });
      return; // Success, exit the loop
    } catch (error) {
      if (error.message.includes("deadlock")) {
        attempt++;
        const delay = Math.pow(2, attempt) * 100 + Math.random() * 100; // Exponential backoff with jitter
        console.warn(`Deadlock detected. Retrying in ${delay}ms (attempt ${attempt}/${maxRetries})`);
        await new Promise((resolve) => setTimeout(resolve, delay));
      } else {
        throw error; // If it's not a deadlock, rethrow the error
      }
    }
  }

  throw new Error("Max retries reached. Transaction failed.");
}
```

---

### **Breaking Down the Code**
- The function **wraps the database operation** inside a transaction.
- If a **deadlock occurs**, it:
  - **Waits for an exponentially increasing delay** (`2^attempt * 100ms`).
  - **Adds jitter** (`+ Math.random() * 100ms`) to prevent synchronized retries.
  - **Retries the transaction**, up to `maxRetries` times.
- If the maximum number of retries is reached, it **throws an error**.

---

### **Why Use Exponential Backoff?**
✅ **Reduces database contention** (prevents simultaneous retries from causing new deadlocks).  
✅ **Increases chances of success** (waits for locks to be released).  
✅ **Prevents unnecessary failures** (retries intelligently instead of failing immediately).  




## What is a two-phase commit? When would you use it?


A **two-phase commit (2PC)** is a distributed transaction protocol used to ensure **atomicity** across multiple databases or systems. It ensures that either all participants in a transaction commit the changes or none of them do, maintaining consistency even in distributed environments.

---

### **How Two-Phase Commit Works**

The two-phase commit protocol has two key phases: 

#### **Phase 1: Prepare Phase**
1. The **coordinator** (a central controlling entity) sends a **prepare message** to all participants (databases or nodes).
2. Each participant:
   - Executes the transaction locally, without committing the changes yet.
   - Checks whether it can successfully commit the changes (e.g., no errors, constraints satisfied).
   - Responds with either:
     - **Vote COMMIT**: Ready to commit.
     - **Vote ABORT**: Unable to commit (e.g., due to errors).

#### **Phase 2: Commit/Abort Phase**
1. The coordinator collects responses from all participants:
   - If **all participants vote COMMIT**, the coordinator sends a **COMMIT message** to all participants, and they commit the transaction.
   - If **any participant votes ABORT**, the coordinator sends an **ABORT message**, and all participants roll back their changes.
2. Each participant acknowledges the decision (commit or abort).

---

### **When to Use Two-Phase Commit**
The two-phase commit protocol is used in scenarios where a transaction spans multiple systems, and **consistency across systems is critical**. Typical use cases include:

1. **Distributed Databases**:
   - When a single transaction involves multiple databases (e.g., updating an order in one database and inventory in another).

2. **Microservices Architectures**:
   - When multiple microservices, each managing its own database, must coordinate a transaction (e.g., reserving a hotel room and booking a flight in a travel application).

3. **Cross-Data Center Transactions**:
   - When data updates span geographically distributed systems that need to remain consistent (e.g., global financial systems).

4. **Eventual Consistency with Atomicity Guarantees**:
   - When you need strong consistency between systems but still operate in a distributed environment.

---

### **Advantages of Two-Phase Commit**
1. **Consistency**:
   - Ensures all or none of the changes are applied across systems.
   
2. **Reliability**:
   - Prevents partial updates, which could leave systems in an inconsistent state.

3. **Compatibility**:
   - Supported by most relational databases (e.g., MySQL, PostgreSQL, Oracle) and middleware systems (e.g., Kafka, RabbitMQ).

---

### **Disadvantages of Two-Phase Commit**
1. **Blocking Nature**:
   - Participants remain locked (waiting for a decision) during the commit process, reducing performance.

2. **Single Point of Failure**:
   - If the coordinator crashes, participants may be left in an uncertain state.

3. **Latency**:
   - Increased transaction latency due to the multiple communication steps.

4. **Resource Intensive**:
   - Requires participants to maintain logs and locks for the duration of the protocol.

---

### **Alternatives to Two-Phase Commit**
If performance or availability is a concern, you might use other approaches:

1. **Eventual Consistency** (e.g., Saga Pattern):
   - Use compensating transactions to roll back changes in a distributed system if an error occurs.

2. **Consensus Algorithms** (e.g., Paxos, Raft):
   - Achieve agreement among distributed systems, often used for distributed databases like Cassandra or CockroachDB.

3. **Distributed Transactions with 3PC**:
   - A three-phase commit adds an extra phase to reduce blocking and uncertainty caused by coordinator failures.

---

### **Summary**
The two-phase commit protocol is ideal for systems where **strong consistency** across distributed components is non-negotiable, such as financial systems or inventory management. However, due to its performance overhead and blocking nature, it's not always the best choice for highly available or low-latency systems, where alternative patterns like eventual consistency or the Saga pattern might be more suitable.


Here’s an example of implementing a **two-phase commit** in a **NestJS application** using a relational database with TypeORM to demonstrate how you can handle distributed transactions.

---

### **Scenario**
Imagine you are building a banking application where a transaction needs to debit an amount from one account and credit it to another. These operations might span two separate services or databases, and you want to ensure that both succeed or neither is applied.

---

### **Code Example**

#### **1. Install Required Dependencies**
Ensure you have **TypeORM** and a database driver installed (e.g., PostgreSQL).

```bash
npm install @nestjs/typeorm typeorm pg
```

---

#### **2. Setup Database Entities**

##### **Account Entity**
Each account has an ID and a balance.

```typescript
// src/account.entity.ts
import { Entity, PrimaryGeneratedColumn, Column } from 'typeorm';

@Entity()
export class Account {
  @PrimaryGeneratedColumn()
  id: number;

  @Column({ type: 'decimal', precision: 10, scale: 2, default: 0 })
  balance: number;
}
```

---

#### **3. Implement Two-Phase Commit Logic**

##### **Service Layer**

```typescript
// src/transaction.service.ts
import { Injectable, InternalServerErrorException } from '@nestjs/common';
import { DataSource, QueryRunner } from 'typeorm';
import { Account } from './account.entity';

@Injectable()
export class TransactionService {
  constructor(private readonly dataSource: DataSource) {}

  async transferFunds(senderId: number, receiverId: number, amount: number): Promise<void> {
    // Create a query runner for managing the transaction
    const queryRunner: QueryRunner = this.dataSource.createQueryRunner();

    // Connect the query runner to the database
    await queryRunner.connect();

    // Start a transaction (Phase 1: Prepare Phase)
    await queryRunner.startTransaction();

    try {
      // Step 1: Debit sender's account
      const sender = await queryRunner.manager.findOne(Account, { where: { id: senderId } });
      if (!sender || sender.balance < amount) {
        throw new Error('Insufficient funds or sender not found');
      }
      sender.balance -= amount;
      await queryRunner.manager.save(sender);

      // Step 2: Credit receiver's account
      const receiver = await queryRunner.manager.findOne(Account, { where: { id: receiverId } });
      if (!receiver) {
        throw new Error('Receiver not found');
      }
      receiver.balance += amount;
      await queryRunner.manager.save(receiver);

      // If everything is successful, commit the transaction (Phase 2: Commit Phase)
      await queryRunner.commitTransaction();
    } catch (error) {
      // If any error occurs, rollback the transaction (Abort Phase)
      await queryRunner.rollbackTransaction();
      throw new InternalServerErrorException(error.message);
    } finally {
      // Release the query runner to free up resources
      await queryRunner.release();
    }
  }
}
```

---

#### **4. Create a Controller**

##### **Transaction Controller**
Expose an endpoint for the transaction.

```typescript
      // src/transaction.controller.ts
      import { Controller, Post, Body } from '@nestjs/common';
      import { TransactionService } from './transaction.service';
      
      @Controller('transactions')
      export class TransactionController {
        constructor(private readonly transactionService: TransactionService) {}
      
        @Post('transfer')
        async transferFunds(@Body() body: { senderId: number; receiverId: number; amount: number }) {
          const { senderId, receiverId, amount } = body;
      
          try {
            await this.transactionService.transferFunds(senderId, receiverId, amount);
            return {
              message: 'Transfer successful',
              senderId,
              receiverId,
              amount,
            };
          } catch (error) {
            return {
              message: 'Transfer failed',
              error: error.message,
            };
          }
        }
      }
```
